{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recommendation System based on Link Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyG to SNAP\n",
    "\n",
    "Load grpah from PyG and convert to SNAP for building a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "import networkx\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graph': HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=61 },\n",
      "  \u001b[1mproduct\u001b[0m={ x=[207, 1153] },\n",
      "  \u001b[1m(user, like, product)\u001b[0m={ edge_index=[2, 207] }\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/marceloyou/Desktop/算法/RecommanationSystem/GNN(GCN)/data'\n",
    "filename = 'graph_0612_20:36:56.pth'\n",
    "\n",
    "graph = torch.load(os.path.join(path, filename))\n",
    "print(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=61 },\n",
      "  \u001b[1mproduct\u001b[0m={ x=[207, 1153] },\n",
      "  \u001b[1m(user, like, product)\u001b[0m={ edge_index=[2, 207] }\n",
      "):\n",
      "======================\n",
      "Number of features: torch.Size([207, 1153])\n",
      "Number of Nodes: 61\n"
     ]
    }
   ],
   "source": [
    "dataset = graph['graph']\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "shape = dataset['product']['x'].shape\n",
    "print(f'Number of features: {shape}')\n",
    "num_node = dataset['user'].num_nodes\n",
    "print(f'Number of Nodes: {num_node}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Edges of the graph: 207\n"
     ]
    }
   ],
   "source": [
    "num = dataset['user','like','product'].edge_index.shape[1]\n",
    "print(f'Number of Edges of the graph: {num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'HeteroData' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zw/7nv8d16j6hg578ch_bvxrqb00000gn/T/ipykernel_7800/553981608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_undirected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch_geometric/utils/convert.py\u001b[0m in \u001b[0;36mto_networkx\u001b[0;34m(data, node_attrs, edge_attrs, to_undirected, remove_self_loops)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_attrs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0medge_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'HeteroData' object is not callable"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "G = to_networkx(dataset, to_undirected = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG to DeepSNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(G=[], edge_index=[2, 10556], edge_label_index=[2, 10556], node_feature=[2708, 1433], node_label=[2708], node_label_index=[2708])\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './tmp/cora'\n",
    "  name = 'Cora'\n",
    "\n",
    "  # The Cora dataset\n",
    "  pyg_dataset= Planetoid(root, name)\n",
    "\n",
    "  # PyG dataset to a list of deepsnap graphs\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  # Get the first deepsnap graph (CORA only has one graph)\n",
    "  graph = graphs[0]\n",
    "  print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Networkx object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "data = dataset.to_homogeneous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nx =to_networkx(data, to_undirected = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data_nx.edges(data = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'user':0, 'product':1}\n",
    "for i, node in enumerate(data_nx.nodes(data = True)):\n",
    "    if i <= 61908:\n",
    "        node[1]['node_label'] = 0\n",
    "        node[1]['node_type'] ='user'\n",
    "        node[1]['node_feature'] = torch.zeros(1153)\n",
    "    else:\n",
    "        node[1]['node_label'] = 1\n",
    "        node[1]['node_feature'] = dataset['product'].x[i-61909]\n",
    "        node[1]['node_type'] = 'product'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small graph\n",
    "label_mapping = {'user':0, 'product':1}\n",
    "for i, node in enumerate(data_nx.nodes(data = True)):\n",
    "    if i <= 60:\n",
    "        node[1]['node_label'] = 0\n",
    "        node[1]['node_type'] ='user'\n",
    "        node[1]['node_feature'] = torch.zeros(1153)\n",
    "    else:\n",
    "        node[1]['node_label'] = 1\n",
    "        node[1]['node_feature'] = dataset['product'].x[i-61]\n",
    "        node[1]['node_type'] = 'product'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v, edge in data_nx.edges(data = True):\n",
    "    data_nx.add_edge(v, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 162, {'edge_label': 0, 'edge_type': 'buy'}),\n",
       " (61, 0, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (61, 3, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (61, 5, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (62, 1, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (63, 2, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (64, 2, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (65, 4, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (66, 6, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (67, 7, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (68, 2, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (69, 2, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (70, 3, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (71, 8, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (72, 2, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (73, 9, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (74, 10, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (75, 11, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (76, 9, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (77, 12, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (78, 12, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (79, 2, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (80, 13, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (81, 14, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (82, 14, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (82, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (83, 14, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (84, 13, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (85, 15, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (86, 15, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (87, 16, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (88, 17, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (89, 6, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (90, 18, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (91, 19, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (92, 20, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (93, 21, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (94, 21, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (95, 17, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (95, 27, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (95, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (95, 53, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 10, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 28, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 35, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 38, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 43, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (96, 50, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (97, 18, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (98, 18, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (99, 22, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (100, 23, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (100, 25, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (101, 24, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (102, 25, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (103, 26, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (103, 38, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (104, 26, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (105, 27, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (106, 28, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (107, 29, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (108, 8, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (108, 30, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (109, 30, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (110, 10, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (110, 31, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (111, 32, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (112, 33, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (113, 34, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (114, 35, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (114, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (115, 36, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (116, 36, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (117, 37, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (118, 37, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (119, 18, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (119, 38, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (120, 38, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (120, 42, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (121, 4, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (122, 39, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (123, 40, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (124, 40, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (125, 41, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (126, 44, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (127, 45, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (128, 38, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (129, 46, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (130, 35, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (131, 47, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (132, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (133, 6, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (133, 25, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (134, 6, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (134, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (135, 49, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (136, 35, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (137, 11, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (138, 51, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (139, 28, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (140, 28, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (141, 52, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (142, 10, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (143, 1, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (144, 1, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (145, 1, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (146, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (147, 54, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (148, 54, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (149, 55, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (150, 35, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (150, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (151, 56, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (152, 57, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (153, 48, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (154, 57, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (155, 57, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (156, 57, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (157, 57, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (158, 57, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (159, 58, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (160, 16, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (161, 59, {'edge_label': 1, 'edge_type': 'attract'}),\n",
       " (162, 60, {'edge_label': 1, 'edge_type': 'attract'})]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_nx.edges(data = True))[123:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, edge in enumerate(data_nx.edges(data = True)):\n",
    "    if i <= 123:\n",
    "        edge[2]['edge_label'] = 0\n",
    "        edge[2]['edge_type'] = 'buy'\n",
    "    else:\n",
    "        edge[2]['edge_label'] = 1\n",
    "        edge[2]['edge_type'] = 'attract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "\n",
    "data_snap = HeteroGraph(data_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.node_type[61909] # 前61908 is user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('user',\n",
       "  'buy',\n",
       "  'product'): tensor([[  0,   1,   1,   1,   1,   2,   2,   2,   2,   2,   2,   3,   3,   4,\n",
       "            4,   5,   6,   6,   6,   6,   7,   8,   8,   9,   9,  10,  10,  10,\n",
       "           10,  11,  11,  12,  12,  13,  13,  14,  14,  14,  15,  15,  16,  16,\n",
       "           17,  17,  18,  18,  18,  18,  19,  20,  21,  21,  22,  23,  24,  25,\n",
       "           25,  25,  26,  26,  27,  27,  28,  28,  28,  28,  29,  30,  30,  31,\n",
       "           32,  33,  34,  35,  35,  35,  35,  35,  36,  36,  37,  37,  38,  38,\n",
       "           38,  38,  38,  39,  40,  40,  41,  42,  43,  44,  45,  46,  47,  48,\n",
       "           48,  48,  48,  48,  48,  48,  48,  48,  49,  50,  51,  52,  53,  54,\n",
       "           54,  55,  56,  57,  57,  57,  57,  57,  57,  58,  59,  60],\n",
       "         [  0,   1,  82,  83,  84,   2,   3,   7,   8,  11,  18,   0,   9,   4,\n",
       "           60,   0,   5,  28,  72,  73,   6,  10,  47,  12,  15,  13,  35,  81,\n",
       "           49,  14,  76,  16,  17,  19,  23,  20,  21,  22,  24,  25,  26,  99,\n",
       "           27,  34,  29,  36,  37,  58,  30,  31,  32,  33,  38,  39,  40,  41,\n",
       "           39,  72,  42,  43,  44,  34,  45,  35,  78,  79,  46,  47,  48,  49,\n",
       "           50,  51,  52,  53,  35,  69,  75,  89,  54,  55,  56,  57,  58,  59,\n",
       "           67,  35,  42,  61,  62,  63,  64,  59,  35,  65,  66,  68,  70,  71,\n",
       "           85,  21,  34,  35,  89,  92,  73,  53,  74,  35,  77,  80,  34,  86,\n",
       "           87,  88,  90,  91,  93,  94,  95,  96,  97,  98, 100, 101]]),\n",
       " ('product',\n",
       "  'attract',\n",
       "  'user'): tensor([[  0,   0,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
       "           12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  21,  22,  23,  24,\n",
       "           25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  34,  34,  34,  35,\n",
       "           35,  35,  35,  35,  35,  35,  36,  37,  38,  39,  39,  40,  41,  42,\n",
       "           42,  43,  44,  45,  46,  47,  47,  48,  49,  49,  50,  51,  52,  53,\n",
       "           53,  54,  55,  56,  57,  58,  58,  59,  59,  60,  61,  62,  63,  64,\n",
       "           65,  66,  67,  68,  69,  70,  71,  72,  72,  73,  73,  74,  75,  76,\n",
       "           77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  89,\n",
       "           90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101],\n",
       "         [  0,   3,   5,   1,   2,   2,   4,   6,   7,   2,   2,   3,   8,   2,\n",
       "            9,  10,  11,   9,  12,  12,   2,  13,  14,  14,  48,  14,  13,  15,\n",
       "           15,  16,  17,   6,  18,  19,  20,  21,  21,  17,  27,  48,  53,  10,\n",
       "           28,  35,  38,  43,  48,  50,  18,  18,  22,  23,  25,  24,  25,  26,\n",
       "           38,  26,  27,  28,  29,   8,  30,  30,  10,  31,  32,  33,  34,  35,\n",
       "           48,  36,  36,  37,  37,  18,  38,  38,  42,   4,  39,  40,  40,  41,\n",
       "           44,  45,  38,  46,  35,  47,  48,   6,  25,   6,  48,  49,  35,  11,\n",
       "           51,  28,  28,  52,  10,   1,   1,   1,  48,  54,  54,  55,  35,  48,\n",
       "           56,  57,  48,  57,  57,  57,  57,  57,  58,  16,  59,  60]])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_snap.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   1,   1,   1,   2,   2,   2,   2,   2,   2,   3,   3,   4,\n",
       "           4,   5,   6,   6,   6,   6,   7,   8,   8,   9,   9,  10,  10,  10,\n",
       "          10,  11,  11,  12,  12,  13,  13,  14,  14,  14,  15,  15,  16,  16,\n",
       "          17,  17,  18,  18,  18,  18,  19,  20,  21,  21,  22,  23,  24,  25,\n",
       "          25,  25,  26,  26,  27,  27,  28,  28,  28,  28,  29,  30,  30,  31,\n",
       "          32,  33,  34,  35,  35,  35,  35,  35,  36,  36,  37,  37,  38,  38,\n",
       "          38,  38,  38,  39,  40,  40,  41,  42,  43,  44,  45,  46,  47,  48,\n",
       "          48,  48,  48,  48,  48,  48,  48,  48,  49,  50,  51,  52,  53,  54,\n",
       "          54,  55,  56,  57,  57,  57,  57,  57,  57,  58,  59,  60],\n",
       "        [  0,   1,  82,  83,  84,   2,   3,   7,   8,  11,  18,   0,   9,   4,\n",
       "          60,   0,   5,  28,  72,  73,   6,  10,  47,  12,  15,  13,  35,  81,\n",
       "          49,  14,  76,  16,  17,  19,  23,  20,  21,  22,  24,  25,  26,  99,\n",
       "          27,  34,  29,  36,  37,  58,  30,  31,  32,  33,  38,  39,  40,  41,\n",
       "          39,  72,  42,  43,  44,  34,  45,  35,  78,  79,  46,  47,  48,  49,\n",
       "          50,  51,  52,  53,  35,  69,  75,  89,  54,  55,  56,  57,  58,  59,\n",
       "          67,  35,  42,  61,  62,  63,  64,  59,  35,  65,  66,  68,  70,  71,\n",
       "          85,  21,  34,  35,  89,  92,  73,  53,  74,  35,  77,  80,  34,  86,\n",
       "          87,  88,  90,  91,  93,  94,  95,  96,  97,  98, 100, 101]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_snap.edge_index[('user','buy','product')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61, 1153])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check each component\n",
    "data_snap.node_feature['user'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on DeepSnap Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many nodes of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of product in the data set is 207\n",
      "The number of user in the data set is 61\n"
     ]
    }
   ],
   "source": [
    "num_node_0 = len(data_snap.node_type['product'])\n",
    "num_node_1 = len(data_snap.node_type['user'])\n",
    "\n",
    "print(f'The number of product in the data set is {num_node_0}')\n",
    "\n",
    "print(f'The number of user in the data set is {num_node_1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many edges are of each message type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message type ('user', 'buy', 'product') has 124 edges\n"
     ]
    }
   ],
   "source": [
    "t = data_snap.message_types[0]\n",
    "message_type_edges = [(t, len(data_snap.edge_type[t]))]\n",
    "\n",
    "for (message_type, num) in message_type_edges:\n",
    "    print(\"Message type {} has {} edges\".format(message_type, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'product': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_snap.node_label #edge_index[('user','buy','product')].t().shape # now the index corresponding to each index in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsnap.dataset import GraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'link_pred'\n",
    "edge_train_mode = 'all'\n",
    "dataset = GraphDataset([data_snap], task = task,  edge_train_mode=edge_train_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 124])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_index[('user','buy','product')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61, 1153])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_train[0].node_feature['user'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marceloyou/opt/anaconda3/lib/python3.8/site-packages/deepsnap/hetero_graph.py:3138: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  row[message_type] = perm[message_type] // num_nodes[tail_type]\n"
     ]
    }
   ],
   "source": [
    "datas_train, datas_valid, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 99])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_train[0].edge_index[('product','attract','user')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'product']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph has 124 edges\n",
      "\n",
      "Train set has 99 message passing edge\n",
      "Train set has 99 supervision (positive) edges\n",
      "\n",
      "Validation set has 99 message passing edge\n",
      "Validation set has 12 supervision (positive) edges\n",
      "\n",
      "Test set has 111 message passing edge\n",
      "Test set has 13 supervision (positive) edges\n"
     ]
    }
   ],
   "source": [
    "  print(\"Original graph has {} edges\".format(dataset[0].edge_index[('user','buy','product')].shape[1]))\n",
    "  print()\n",
    "\n",
    "  print(\"Train set has {} message passing edge\".format(datas_train[0].edge_index[('user','buy','product')].shape[1]))\n",
    "  print(\"Train set has {} supervision (positive) edges\".format(datas_train[0].edge_label_index[('user','buy','product')].shape[1] // 2))\n",
    "\n",
    "  print()\n",
    "  print(\"Validation set has {} message passing edge\".format(datas_valid[0].edge_index[('user','buy','product')].shape[1]))\n",
    "  print(\"Validation set has {} supervision (positive) edges\".format(datas_valid[0].edge_label_index[('user','buy','product')].shape[1] // 2))\n",
    "\n",
    "  print()\n",
    "  print(\"Test set has {} message passing edge\".format(dataset_test[0].edge_index[('user','buy','product')].shape[1]))\n",
    "  print(\"Test set has {} supervision (positive) edges\".format(dataset_test[0].edge_label_index[('user','buy','product')].shape[1] // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data set checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive and nagative edges:  True\n"
     ]
    }
   ],
   "source": [
    "def check_disjoint(edge_index1, edge_index2):\n",
    "    edge_index1 = edge_index1.detach().numpy()\n",
    "    edge_index2 = edge_index2.detach().numpy()\n",
    "    \n",
    "    intercept = [x for x in set(tuple(x) for x in edge_index1) &\n",
    "                   set(tuple(x) for x in edge_index2)]\n",
    "    disjoint = len(intercept) == 0\n",
    "    return disjoint\n",
    "\n",
    "num_train_edges = datas_train[0].edge_label_index[('user','buy','product')].shape[1] // 2\n",
    "train_index1 =  datas_train[0].edge_label_index[('user','buy','product')][:, :num_train_edges]\n",
    "train_index2 =  datas_train[0].edge_label_index[('user','buy','product')][:num_train_edges]\n",
    "\n",
    "print('Postive and nagative edges: ', check_disjoint(train_index1, train_index2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In normal settings, the negative sample ratio is normally 1.Negative samples will be resampled when batch is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the negative sampling of two batch\n",
      "Batch1: \n",
      " tensor([[ 53307,  11193,  59911,  ...,  54221,  38121,   2011],\n",
      "        [181166, 111196,  47537,  ...,  44294,  62045,   8673]])\n",
      "Batch2: \n",
      " tensor([[ 48890,  46336,  30079,  ...,  27309,  36820,  50808],\n",
      "        [ 46779, 215916,  56737,  ...,  26691,  66502,  70872]])\n",
      "Whether Batch1 equal to batch2:  False\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "follow_batch = []\n",
    "from torch.utils.data import DataLoader\n",
    "datasets['train'],datasets['valid'], dataset['test'] = datas_train,datas_valid, dataset_test\n",
    "dataloaders = {\n",
    "    split: DataLoader(ds, collate_fn = Batch.collate(follow_batch),\n",
    "                     batch_size = 1, shuffle = (split == 'train'))\n",
    "    for split, ds in datasets.items()\n",
    "}\n",
    "\n",
    "print(\"check the negative sampling of two batch\")\n",
    "\n",
    "for batch in dataloaders['train']:\n",
    "    num_edges = batch.edge_label_index[('user','buy','product')].shape[1] // 2\n",
    "    neg_edge_1 = batch.edge_label_index[('user','buy','product')][:, num_edges:]\n",
    "    \n",
    "for batch in dataloaders['valid']:\n",
    "    num_edges = batch.edge_label_index[('user','buy','product')].shape[1] // 2\n",
    "    neg_edge_2 = batch.edge_label_index[('user','buy','product')][:, num_edges:]\n",
    "    \n",
    "print('Batch1: \\n', neg_edge_1)\n",
    "print('Batch2: \\n', neg_edge_2)\n",
    "print('Whether Batch1 equal to batch2: ', torch.equal(neg_edge_1, neg_edge_2) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 1154\n"
     ]
    }
   ],
   "source": [
    "print('Before:', dataset[0].node_feature['user'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset\n",
    "pr = nx.pagerank(data_nx)\n",
    "node_num = len(list(data_nx.nodes(data = True)))\n",
    "pr_feature = torch.tensor([pr[node] for node in range(node_num)], dtype = torch.float32).view(node_num, 1)\n",
    "\n",
    "user_num = dataset[0].node_feature['user'].shape[0]\n",
    "dataset[0].node_feature['user'] = torch.cat([dataset[0].node_feature['user'], pr_feature[:user_num]], dim = 1)\n",
    "\n",
    "dataset[0].node_feature['product'] = torch.cat([dataset[0].node_feature['product'], pr_feature[user_num:]], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Transformation: \n",
      "After:  torch.Size([61, 1154])\n"
     ]
    }
   ],
   "source": [
    "print('Check Transformation: ')\n",
    "#print('Before: ', dataset2[0].node_feature['user'].shape)\n",
    "print('After: ', dataset[0].node_feature['user'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.903447610934537e-06"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, when we feed each batch, it has two layers of convolution layer, we select the source and destination embeddings, we multiply and then sum up for our final results. When predict, we feed another sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn as nn\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "from deepsnap.hetero_gnn import (\n",
    "    HeteroSAGEConv,\n",
    "    HeteroConv,\n",
    "    forward_op\n",
    ")\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeteroGNNConv Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heterogenos graph has twp types of node user and product, the message type is (u, buy, product). Since there is only one type of link, there is only one kind of heterogenous message layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2convs_link_pred_layers(hete, conv, hidden_size):\n",
    "    convs1 = {}\n",
    "    convs2 = {}\n",
    "    for message_type in hete.message_types:\n",
    "        s_type = message_type[0]\n",
    "        d_type = message_type[2]\n",
    "        s_feed_dim = hete.num_node_features(s_type)\n",
    "        d_feed_dim = hete.num_node_features(d_type)\n",
    "        convs1[message_type] = conv(s_feed_dim, hidden_size, d_feed_dim)\n",
    "        convs2[message_type] = conv(hidden_size,hidden_size,hidden_size)\n",
    "    return convs1, convs2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 'buy', 'product'), ('product', 'attract', 'user')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_train[0].message_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPrediction(nn.Module):\n",
    "    def __init__(self,args, conv1, conv2, hetero):\n",
    "        super(LinkPrediction, self).__init__()\n",
    "        self.agrs = args\n",
    "        self.convs1 = HeteroConv(conv1)\n",
    "        self.convs2 = HeteroConv(conv2)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        \n",
    "        for node_type in hetero.node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(args['hidden_size'])\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(args['hidden_size'])\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = data.node_feature\n",
    "        #print(x)\n",
    "        edge_index = data.edge_index\n",
    "        x = self.convs1(x, edge_index)\n",
    "        #print(x)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        #print(x)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        pred = {}\n",
    "        \n",
    "        for message_type in list(data.edge_index.keys()):\n",
    "            if message_type == ('user', 'buy', 'product'):\n",
    "                nodes_first = torch.index_select(x['user'], 0, data.edge_label_index[message_type][0,:].long())\n",
    "                nodes_second = torch.index_select(x['product'], 0, data.edge_label_index[message_type][1,:].long())\n",
    "\n",
    "                pred[message_type] = torch.sum(nodes_first * nodes_second, dim = -1)\n",
    "            else:\n",
    "                pass\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, pred, y):\n",
    "        loss = 0\n",
    "        for key in pred:\n",
    "    \n",
    "             p = torch.sigmoid(pred[key])\n",
    "             #print(p, pred[key])\n",
    "             y = torch.tensor([i.item() - 1 if i.item() == 2 else i.item() for i in y[key] ])\n",
    " \n",
    "             loss += self.loss_fn(p, y.type(pred[key].dtype))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    \n",
    "conv1, conv2 = generate_2convs_link_pred_layers(dataset[0], HeteroSAGEConv, 256)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, optimizer, args):\n",
    "    val_max = 0\n",
    "    best_moedl = model\n",
    "    t_accu, v_accu, e_accu = [], [], []\n",
    "    losses = []\n",
    "    for epoch in range(1, args['epoches'] + 1):\n",
    "        for iter_i, batch in enumerate(dataloaders['train']):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch)\n",
    "            loss = model.loss(pred, batch.edge_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "            log = 'Epoch: {:03d}, Train loss: {:.4f}ff}'\n",
    "            accs = test(model, dataloaders, args)\n",
    "            t_accu.append(accs['train'])\n",
    "            v_accu.append(accs['valid'])\n",
    "            e_accu.append(accs['test'])\n",
    "            \n",
    "            print(log.format(epoch, loss.item(), accs['train'],accs['valid'],accs['test']))\n",
    "            if val_max < accs['valid']:\n",
    "                val_max = accs['valid']\n",
    "                best_model = deepcopy(model)\n",
    "    log = 'Best: Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    accs = test(best_model, dataloaders, args)\n",
    "    print(log.format(accs['train'], accs['valid'], accs['test']))\n",
    "        \n",
    "    return t_accu, v_accu, e_accu, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloaders, args):\n",
    "    model.eval()\n",
    "    \n",
    "    accs = {}\n",
    "    \n",
    "    for mode, dataloader in dataloaders.items():\n",
    "        acc = 0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            num = 0\n",
    "            pred = model(batch)\n",
    "            \n",
    "            for key in pred:\n",
    "                p = torch.sigmoid(pred[key]).detach().numpy()\n",
    "                pred_label = np.zeros_like(p, dtype = np.float64)\n",
    "                pred_label[np.where(p > 0.5)[0]] = 1\n",
    "                pred_label[np.where(p < 0.5)[0]] = 0\n",
    "                acc += np.sum(pred_label == batch.edge_label[key].numpy())\n",
    "                num += len(pred_label)\n",
    "        accs[mode] = acc / num\n",
    "    return accs\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def main():\n",
    "    \n",
    "    args = {'epoches': 200,\n",
    "           'hidden_size': 128,\n",
    "            'path':'/Users/marceloyou/Desktop/算法/RecommanationSystem/GNN(GCN)/src/',\n",
    "           'output_path': time.strftime('checkpoints/model_' + '%m%d_%H:%M:%S.pth') }\n",
    "    \n",
    "    train_loader = DataLoader(datas_train, collate_fn = Batch.collate(), batch_size = 1)\n",
    "    valid_loader = DataLoader(datas_valid, collate_fn = Batch.collate(), batch_size = 1)\n",
    "    test_loader = DataLoader(datas_train, collate_fn = Batch.collate(), batch_size = 1)\n",
    "    \n",
    "    dataloaders = {'train': train_loader, 'valid':valid_loader, 'test':test_loader}\n",
    "    hidden_size = args['hidden_size']\n",
    "    \n",
    "    conv1, conv2 = generate_2convs_link_pred_layers(dataset[0], HeteroSAGEConv, hidden_size)\n",
    "    model = LinkPrediction(args, conv1, conv2, dataset[0])    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "    t_accu, v_accu, e_accu, best_model = train(model, dataloaders, optimizer, args)\n",
    "    \n",
    "    model = {'model':model.state_dict()}\n",
    "    torch.save(model, os.path.join(args['path'], args['output_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train loss: 0.7846, Train: 0.4293, Val: 0.1667, Test: 0.4293\n",
      "Epoch: 002, Train loss: 0.6529, Train: 0.4394, Val: 0.3333, Test: 0.4394\n",
      "Epoch: 003, Train loss: 0.6117, Train: 0.4646, Val: 0.3333, Test: 0.4646\n",
      "Epoch: 004, Train loss: 0.6007, Train: 0.4798, Val: 0.3333, Test: 0.4798\n",
      "Epoch: 005, Train loss: 0.6199, Train: 0.4798, Val: 0.3333, Test: 0.4798\n",
      "Epoch: 006, Train loss: 0.6096, Train: 0.4747, Val: 0.3750, Test: 0.4747\n",
      "Epoch: 007, Train loss: 0.6236, Train: 0.4747, Val: 0.3750, Test: 0.4747\n",
      "Epoch: 008, Train loss: 0.6116, Train: 0.4899, Val: 0.3750, Test: 0.4899\n",
      "Epoch: 009, Train loss: 0.6221, Train: 0.4899, Val: 0.3333, Test: 0.4899\n",
      "Epoch: 010, Train loss: 0.6016, Train: 0.4899, Val: 0.2917, Test: 0.4899\n",
      "Epoch: 011, Train loss: 0.6148, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 012, Train loss: 0.6065, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 013, Train loss: 0.6046, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 014, Train loss: 0.5918, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 015, Train loss: 0.6078, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 016, Train loss: 0.6156, Train: 0.4949, Val: 0.2500, Test: 0.4949\n",
      "Epoch: 017, Train loss: 0.5930, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 018, Train loss: 0.6131, Train: 0.4899, Val: 0.1667, Test: 0.4899\n",
      "Epoch: 019, Train loss: 0.5955, Train: 0.4899, Val: 0.1667, Test: 0.4899\n",
      "Epoch: 020, Train loss: 0.6311, Train: 0.4899, Val: 0.1667, Test: 0.4899\n",
      "Epoch: 021, Train loss: 0.6014, Train: 0.4899, Val: 0.1667, Test: 0.4899\n",
      "Epoch: 022, Train loss: 0.5955, Train: 0.4899, Val: 0.1667, Test: 0.4899\n",
      "Epoch: 023, Train loss: 0.5992, Train: 0.4848, Val: 0.1667, Test: 0.4848\n",
      "Epoch: 024, Train loss: 0.5972, Train: 0.4646, Val: 0.2083, Test: 0.4646\n",
      "Epoch: 025, Train loss: 0.5909, Train: 0.4646, Val: 0.2083, Test: 0.4646\n",
      "Epoch: 026, Train loss: 0.5927, Train: 0.4646, Val: 0.2083, Test: 0.4646\n",
      "Epoch: 027, Train loss: 0.6145, Train: 0.4646, Val: 0.2083, Test: 0.4646\n",
      "Epoch: 028, Train loss: 0.6010, Train: 0.4646, Val: 0.1667, Test: 0.4646\n",
      "Epoch: 029, Train loss: 0.5989, Train: 0.4646, Val: 0.1667, Test: 0.4646\n",
      "Epoch: 030, Train loss: 0.6127, Train: 0.4646, Val: 0.1667, Test: 0.4646\n",
      "Epoch: 031, Train loss: 0.5930, Train: 0.4596, Val: 0.1667, Test: 0.4596\n",
      "Epoch: 032, Train loss: 0.5975, Train: 0.4899, Val: 0.2083, Test: 0.4899\n",
      "Epoch: 033, Train loss: 0.5928, Train: 0.5000, Val: 0.2917, Test: 0.5000\n",
      "Epoch: 034, Train loss: 0.6092, Train: 0.4949, Val: 0.3333, Test: 0.4949\n",
      "Epoch: 035, Train loss: 0.5919, Train: 0.4899, Val: 0.3333, Test: 0.4899\n",
      "Epoch: 036, Train loss: 0.5977, Train: 0.4848, Val: 0.3333, Test: 0.4848\n",
      "Epoch: 037, Train loss: 0.5905, Train: 0.4848, Val: 0.3333, Test: 0.4848\n",
      "Epoch: 038, Train loss: 0.6164, Train: 0.4747, Val: 0.3333, Test: 0.4747\n",
      "Epoch: 039, Train loss: 0.6122, Train: 0.4697, Val: 0.3333, Test: 0.4697\n",
      "Epoch: 040, Train loss: 0.6223, Train: 0.4747, Val: 0.3750, Test: 0.4747\n",
      "Epoch: 041, Train loss: 0.6032, Train: 0.4747, Val: 0.3750, Test: 0.4747\n",
      "Epoch: 042, Train loss: 0.5968, Train: 0.4798, Val: 0.3750, Test: 0.4798\n",
      "Epoch: 043, Train loss: 0.6150, Train: 0.4848, Val: 0.3750, Test: 0.4848\n",
      "Epoch: 044, Train loss: 0.6033, Train: 0.4848, Val: 0.3750, Test: 0.4848\n",
      "Epoch: 045, Train loss: 0.5917, Train: 0.4848, Val: 0.3750, Test: 0.4848\n",
      "Epoch: 046, Train loss: 0.5905, Train: 0.4899, Val: 0.3750, Test: 0.4899\n",
      "Epoch: 047, Train loss: 0.5996, Train: 0.4899, Val: 0.3750, Test: 0.4899\n",
      "Epoch: 048, Train loss: 0.5878, Train: 0.4848, Val: 0.3750, Test: 0.4848\n",
      "Epoch: 049, Train loss: 0.5910, Train: 0.4798, Val: 0.3333, Test: 0.4798\n",
      "Epoch: 050, Train loss: 0.5810, Train: 0.4798, Val: 0.3333, Test: 0.4798\n",
      "Epoch: 051, Train loss: 0.5975, Train: 0.4798, Val: 0.2917, Test: 0.4798\n",
      "Epoch: 052, Train loss: 0.5820, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 053, Train loss: 0.5979, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 054, Train loss: 0.5937, Train: 0.4899, Val: 0.2917, Test: 0.4899\n",
      "Epoch: 055, Train loss: 0.5927, Train: 0.4899, Val: 0.2917, Test: 0.4899\n",
      "Epoch: 056, Train loss: 0.5989, Train: 0.4899, Val: 0.2917, Test: 0.4899\n",
      "Epoch: 057, Train loss: 0.5932, Train: 0.4899, Val: 0.2917, Test: 0.4899\n",
      "Epoch: 058, Train loss: 0.6002, Train: 0.4899, Val: 0.3333, Test: 0.4899\n",
      "Epoch: 059, Train loss: 0.5810, Train: 0.4899, Val: 0.3333, Test: 0.4899\n",
      "Epoch: 060, Train loss: 0.5859, Train: 0.4949, Val: 0.3333, Test: 0.4949\n",
      "Epoch: 061, Train loss: 0.5900, Train: 0.4798, Val: 0.2917, Test: 0.4798\n",
      "Epoch: 062, Train loss: 0.5966, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 063, Train loss: 0.5878, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 064, Train loss: 0.5907, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 065, Train loss: 0.5852, Train: 0.4495, Val: 0.2500, Test: 0.4495\n",
      "Epoch: 066, Train loss: 0.5951, Train: 0.4545, Val: 0.2500, Test: 0.4545\n",
      "Epoch: 067, Train loss: 0.5852, Train: 0.4646, Val: 0.2917, Test: 0.4646\n",
      "Epoch: 068, Train loss: 0.5795, Train: 0.4798, Val: 0.2917, Test: 0.4798\n",
      "Epoch: 069, Train loss: 0.5757, Train: 0.4798, Val: 0.3333, Test: 0.4798\n",
      "Epoch: 070, Train loss: 0.5874, Train: 0.4747, Val: 0.3333, Test: 0.4747\n",
      "Epoch: 071, Train loss: 0.5820, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 072, Train loss: 0.5846, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 073, Train loss: 0.5874, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 074, Train loss: 0.5888, Train: 0.4848, Val: 0.2083, Test: 0.4848\n",
      "Epoch: 075, Train loss: 0.5619, Train: 0.4848, Val: 0.2083, Test: 0.4848\n",
      "Epoch: 076, Train loss: 0.5895, Train: 0.4848, Val: 0.2083, Test: 0.4848\n",
      "Epoch: 077, Train loss: 0.5920, Train: 0.4848, Val: 0.2083, Test: 0.4848\n",
      "Epoch: 078, Train loss: 0.5855, Train: 0.4848, Val: 0.2083, Test: 0.4848\n",
      "Epoch: 079, Train loss: 0.5689, Train: 0.4848, Val: 0.2083, Test: 0.4848\n",
      "Epoch: 080, Train loss: 0.5741, Train: 0.4798, Val: 0.2917, Test: 0.4798\n",
      "Epoch: 081, Train loss: 0.5836, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 082, Train loss: 0.5663, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 083, Train loss: 0.5699, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 084, Train loss: 0.5852, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 085, Train loss: 0.5716, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 086, Train loss: 0.5654, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 087, Train loss: 0.5698, Train: 0.4747, Val: 0.2917, Test: 0.4747\n",
      "Epoch: 088, Train loss: 0.5781, Train: 0.4848, Val: 0.3750, Test: 0.4848\n",
      "Epoch: 089, Train loss: 0.5679, Train: 0.4848, Val: 0.3750, Test: 0.4848\n",
      "Epoch: 090, Train loss: 0.5789, Train: 0.4798, Val: 0.3750, Test: 0.4798\n",
      "Epoch: 091, Train loss: 0.5601, Train: 0.4798, Val: 0.3333, Test: 0.4798\n",
      "Epoch: 092, Train loss: 0.5727, Train: 0.4848, Val: 0.2917, Test: 0.4848\n",
      "Epoch: 093, Train loss: 0.5506, Train: 0.4798, Val: 0.2917, Test: 0.4798\n",
      "Epoch: 094, Train loss: 0.5582, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 095, Train loss: 0.5585, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 096, Train loss: 0.5586, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 097, Train loss: 0.5519, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 098, Train loss: 0.5622, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 099, Train loss: 0.5610, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 100, Train loss: 0.5574, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 101, Train loss: 0.5581, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 102, Train loss: 0.5564, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 103, Train loss: 0.5585, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 104, Train loss: 0.5641, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 105, Train loss: 0.5566, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 106, Train loss: 0.5618, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 107, Train loss: 0.5658, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 108, Train loss: 0.5547, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 109, Train loss: 0.5720, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 110, Train loss: 0.5504, Train: 0.4596, Val: 0.2500, Test: 0.4596\n",
      "Epoch: 111, Train loss: 0.5695, Train: 0.4495, Val: 0.2500, Test: 0.4495\n",
      "Epoch: 112, Train loss: 0.5616, Train: 0.4444, Val: 0.2500, Test: 0.4444\n",
      "Epoch: 113, Train loss: 0.5569, Train: 0.4293, Val: 0.2500, Test: 0.4293\n",
      "Epoch: 114, Train loss: 0.5513, Train: 0.4141, Val: 0.2500, Test: 0.4141\n",
      "Epoch: 115, Train loss: 0.5497, Train: 0.4141, Val: 0.2500, Test: 0.4141\n",
      "Epoch: 116, Train loss: 0.5630, Train: 0.4192, Val: 0.2500, Test: 0.4192\n",
      "Epoch: 117, Train loss: 0.5546, Train: 0.4242, Val: 0.2500, Test: 0.4242\n",
      "Epoch: 118, Train loss: 0.5578, Train: 0.4343, Val: 0.2500, Test: 0.4343\n",
      "Epoch: 119, Train loss: 0.5640, Train: 0.4596, Val: 0.2500, Test: 0.4596\n",
      "Epoch: 120, Train loss: 0.5773, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 121, Train loss: 0.5696, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 122, Train loss: 0.5580, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 123, Train loss: 0.5537, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 124, Train loss: 0.5573, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 125, Train loss: 0.5678, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 126, Train loss: 0.5581, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 127, Train loss: 0.5538, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 128, Train loss: 0.5573, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 129, Train loss: 0.5565, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 130, Train loss: 0.5520, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 131, Train loss: 0.5514, Train: 0.4596, Val: 0.2500, Test: 0.4596\n",
      "Epoch: 132, Train loss: 0.5540, Train: 0.4596, Val: 0.2500, Test: 0.4596\n",
      "Epoch: 133, Train loss: 0.5624, Train: 0.4596, Val: 0.2500, Test: 0.4596\n",
      "Epoch: 134, Train loss: 0.5461, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 135, Train loss: 0.5508, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 136, Train loss: 0.5487, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 137, Train loss: 0.5498, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 138, Train loss: 0.5604, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 139, Train loss: 0.5497, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 140, Train loss: 0.5610, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 141, Train loss: 0.5581, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 142, Train loss: 0.5535, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 143, Train loss: 0.5528, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 144, Train loss: 0.5521, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 145, Train loss: 0.5616, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 146, Train loss: 0.5613, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 147, Train loss: 0.5557, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 148, Train loss: 0.5600, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 149, Train loss: 0.5537, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 150, Train loss: 0.5544, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 151, Train loss: 0.5461, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 152, Train loss: 0.5531, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 153, Train loss: 0.5481, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 154, Train loss: 0.5646, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 155, Train loss: 0.5344, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 156, Train loss: 0.5477, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 157, Train loss: 0.5472, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 158, Train loss: 0.5547, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 159, Train loss: 0.5516, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 160, Train loss: 0.5510, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 161, Train loss: 0.5455, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 162, Train loss: 0.5696, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 163, Train loss: 0.5659, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 164, Train loss: 0.5719, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 165, Train loss: 0.5512, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 166, Train loss: 0.5437, Train: 0.4899, Val: 0.2500, Test: 0.4899\n",
      "Epoch: 167, Train loss: 0.5551, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 168, Train loss: 0.5521, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 169, Train loss: 0.5519, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 170, Train loss: 0.5671, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 171, Train loss: 0.5578, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 172, Train loss: 0.5456, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 173, Train loss: 0.5517, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 174, Train loss: 0.5597, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 175, Train loss: 0.5639, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 176, Train loss: 0.5592, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 177, Train loss: 0.5540, Train: 0.4596, Val: 0.2500, Test: 0.4596\n",
      "Epoch: 178, Train loss: 0.5666, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 179, Train loss: 0.5581, Train: 0.4596, Val: 0.2917, Test: 0.4596\n",
      "Epoch: 180, Train loss: 0.5617, Train: 0.4596, Val: 0.2917, Test: 0.4596\n",
      "Epoch: 181, Train loss: 0.5573, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 182, Train loss: 0.5604, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 183, Train loss: 0.5550, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 184, Train loss: 0.5562, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 185, Train loss: 0.5492, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 186, Train loss: 0.5780, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 187, Train loss: 0.5547, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 188, Train loss: 0.5529, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 189, Train loss: 0.5535, Train: 0.4848, Val: 0.2500, Test: 0.4848\n",
      "Epoch: 190, Train loss: 0.5529, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 191, Train loss: 0.5560, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 192, Train loss: 0.5611, Train: 0.4798, Val: 0.2500, Test: 0.4798\n",
      "Epoch: 193, Train loss: 0.5516, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 194, Train loss: 0.5540, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 195, Train loss: 0.5462, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 196, Train loss: 0.5634, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 197, Train loss: 0.5594, Train: 0.4646, Val: 0.2500, Test: 0.4646\n",
      "Epoch: 198, Train loss: 0.5532, Train: 0.4697, Val: 0.2500, Test: 0.4697\n",
      "Epoch: 199, Train loss: 0.5588, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Epoch: 200, Train loss: 0.5552, Train: 0.4747, Val: 0.2500, Test: 0.4747\n",
      "Best: Train: 0.4747, Val: 0.3750, Test: 0.4747\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 'buy', 'product'), ('product', 'attract', 'user')]\n",
      "{('user', 'buy', 'product'): tensor([[ 48,   2,  57,  57,  39,  50,  21,  38,  16,  17,  40,  57,  14,  35,\n",
      "           0,  28,  56,  29,  11,  18,  48,  26,  30,  28,  12,  47,   9,  42,\n",
      "           2,  25,  27,  14,  48,  48,  48,   1,  23,  35,  18,  36,  10,  14,\n",
      "           1,  38,  31,  28,  60,  48,   6,  38,  48,  40,  48,  58,   1,  30,\n",
      "           2,  17,  35,  36,  48,   6,  51,  15,   2,   5,  12,  35,  57,  35,\n",
      "          22,  59,  34,  10,   8,   4,  32,  13,  46,  18,   2,   7,  13,  54,\n",
      "          28,  38,  38,  49,  57,  10,  43,   6,  24,  20,  15,   6,  45,  55,\n",
      "           1,  40,   9,   1,  20,  12,  12,   8,  27,  18,   2,   0,   3,  53,\n",
      "           0,  51,  52,   0,   7,  12,  40,   0,  25,  16,  11,  27,  18,  33,\n",
      "          52,  38,  39,  39,  34,  47,  51,  35,  17,  58,  10,   6,  39,   7,\n",
      "          41,  35,  44,  48,  41,  16,  54,   5,   1,  34,  19,  41,  17,  56,\n",
      "           4,   0,  17,  14,  39,  17,  59,  58,  56,  31,  12,  51,  35,  27,\n",
      "          19,  40,  13,  36,  49,  45,   8,  42,  53,  60,  45,  34,  46,  48,\n",
      "          32,  16,  34,  17,  10,   1,  23,  60,  38,  41,  59,   3,  26,  35,\n",
      "          38,  31],\n",
      "        [ 73,  11,  93,  97,  61,  35,  32,  59,  99,  34,  62,  91,  20,  75,\n",
      "           0,  35,  90,  46,  14,  58,  71,  43,  48,  79,  16,  70,  12,  59,\n",
      "           2,  41,  34,  22,  89,  53,  35,  84,  39,  69,  29,  54,  35,  21,\n",
      "           1,  58,  49,  45, 101,  34,  72,  42,  21,  63,  85,  98,  83,  47,\n",
      "           3,  27,  53,  55,  92,   5,  77,  24,   7,   0,  17,  35,  96,  89,\n",
      "          38, 100,  52,  49,  10,   4,  50,  23,  68,  37,   8,   6,  19,  86,\n",
      "          78,  35,  67,  74,  95,  81,  35,  73,  40,  31,  25,  28,  66,  88,\n",
      "          82, 158,  59, 181, 155, 105, 124,  43, 143, 123,  30, 173,  32,  13,\n",
      "         190, 203, 169, 191, 128,  45,  10,  16,  84, 180, 142, 161, 132, 153,\n",
      "         143, 126, 121, 136, 200,  81,  24, 115, 167,  67, 143,  30, 131, 196,\n",
      "          23,  15,  57,  58, 169, 112, 121, 121,   6,  19,  38, 152,  93, 206,\n",
      "          67, 163, 141,  41, 202,  42, 202,  53,  62,  18,  97, 193, 195,  79,\n",
      "         113, 106,   9, 159,  54,  74,  98, 200, 193,  96,  17,  39, 145,  61,\n",
      "         149, 116, 106, 126,  45, 139,  47,   6,  34,  44,  98, 109,  21, 102,\n",
      "           7,  55]]), ('product', 'attract', 'user'): tensor([[ 34,  89,  56,  53,  73,  73,  63,  61,  34,  72,  54,  76,  94,  34,\n",
      "          83,  10,  27,  11,  91,   5,  80,  79,   0,   2,  20,  53,  35,  35,\n",
      "           0,  35,  25,  77,  18,   4,  15,  26,  59,  35,  39,  35,  87,  65,\n",
      "          33,  90,  45,  30,  52,  95,  16,  47,  86,   9,   0,  46,  32,  21,\n",
      "          97,  34,  29,  55,  64,  39,  71,  57,  58,  37,  67,  82,  75,  41,\n",
      "          48,  31,  42,  96,  98,  40, 100,  93, 101,   8,  59,  21,  51,  58,\n",
      "          13,  44,  99,  60,  35,  19,  42,  72,  78,  85,  12,  66,  17,  22,\n",
      "          14, 117, 122,  89, 203,  37, 106, 204,   0,  50, 175, 140,  17, 197,\n",
      "         170,  34,  10,  83, 188,  28, 144,  86,   8, 112, 113,  17, 134, 157,\n",
      "          65, 132, 205,  66, 185, 150,  21,  72,  97,  34,  92, 170,  39,  86,\n",
      "          29, 166,  18,   7,  30,  50,  93,  13, 156, 105,  74,   2,   9,  21,\n",
      "          17,  38,  78, 203, 113, 135, 107,  69,  32, 187, 203,  49, 175,  87,\n",
      "          88, 119, 158,  51,  82,   2,  88,  65,  98,  84, 184, 125, 107, 196,\n",
      "          92,  26, 127,   9,  75,  65,  51,  55,  37,  13, 139,  91,   2,  89,\n",
      "         140, 117],\n",
      "        [ 48,  48,  37,  48,  48,   6,  40,  39,  27,   6,  36,  11,  57,  17,\n",
      "           1,   8,  17,   2,  57,   6,  52,  28,   5,   2,  14,  35,  43,  35,\n",
      "           3,  38,  15,  51,   2,   4,   9,  16,  38,  50,  23,  10,  54,  44,\n",
      "          21,  56,  28,  19,  34,  57,  12,  30,  54,   3,   0,  29,  21,  14,\n",
      "          57,  53,  18,  36,  41,  25,  48,  37,  38,  18,  38,   1,  35,  25,\n",
      "          30,  20,  26,  57,  58,  24,  59,  57,  60,   2,  42,  48,  33,  18,\n",
      "          10,  27,  16,   4,  28,  13,  38,  25,  28,  48,   9,  45,  12,  14,\n",
      "          11,   0,  11,  59,  22,  10,  26,  15,  48,  47,  25,  44,  38,  60,\n",
      "          41,  30,   7,  31,  53,  35,  47,  16,  13,  57,   3,  45,  52,   3,\n",
      "          14,  29,  21,  60,  28,  39,  11,  60,  21,  45,  27,   8,  52,   0,\n",
      "          51,  43,  56,  26,   8,  16,  15,  12,  15,   5,   1,   4,   7,  35,\n",
      "          13,   2,  27,  12,  31,  17,  23,  37,   8,  49,  60,   5,  19,  56,\n",
      "          30,   7,  58,  37,  28,  57,  48,  26,  32,  26,   4,  36,  28,  36,\n",
      "          11,   6,  16,   4,  52,  29,   0,  33,  26,  38,   8,   4,  28,  39,\n",
      "          21,  24]])}\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(datas_train, collate_fn = Batch.collate(), batch_size = 1)\n",
    "for batch in train_loader:\n",
    "    print(list(batch.edge_index.keys()))\n",
    "    print(batch.edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('user',\n",
       "  'buy',\n",
       "  'product'): tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2]),\n",
       " ('product',\n",
       "  'attract',\n",
       "  'user'): tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2])}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_train[0].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
